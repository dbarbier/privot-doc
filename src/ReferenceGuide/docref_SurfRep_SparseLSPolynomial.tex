% Copyright (c)  2005-2010 EDF-EADS-PHIMECA.
% Permission is granted to copy, distribute and/or modify this document
% under the terms of the GNU Free Documentation License, Version 1.2
% or any later version published by the Free Software Foundation;
% with no Invariant Sections, no Front-Cover Texts, and no Back-Cover
% Texts.  A copy of the license is included in the section entitled "GNU
% Free Documentation License".
\renewcommand{\etapemethodo}{Resp. Surf.}
\renewcommand{\nomfichier}{docref_SurfRep_SparseLSPolynomial}
\renewcommand{\titrefiche}{Polynomial response surface based on sparse least squares}

\Header



\MathematicalDescription{
  
  \underline{\textbf{Goal}} \vspace{2mm}
  
The response of the model under consideration may be globally approximated by a multivariate polynomial. In this setup, the response is characterized by a finite number of coefficients which have to be estimated. This may be achieved by least squares (see \otref{docref_SurfRep_LSPolynomial}{Polynomial response surface based on least squares}). However, this method cannot be applied if the number of unknown coefficients is of similar size to the number of model evaluations, or even possibly greater. Indeed, the resulting underdetermined least squares problem would be ill-posed. \\

To overcome this difficulty, \emph{sparse least squares} schemes may be employed (they are also known as \emph{variable selection} techniques in statistics). These methods are aimed at identifying a small set of significant coefficients in the model response approximation. Eventually, a \emph{sparse} polynomial response surface, i.e. a polynomial which only contains a low number of non-zero coefficients, is obtained by means of a reduced number of possibly costly model evaluations. In the sequel, we focus on sparse regression schemes based on $L_1$-penalization. \\

Actually the proposed approaches do not provide only one approximation, but a \emph{collection} of less and less sparse approximations. Eventually an optimal approximation may be retained with regard to a given criterion.
  
  \underline{\textbf{Principles}} \vspace{4mm}

\textbf{Context} \vspace{2mm}

Consider the mathematical model $h$ of a physical system depending on $n_X$ input parameters $\underline{x} = (x_{1},\dots,x_{n_X})^{\textsf{T}}$. Note that these input variables are assumed to be deterministic in this section. The model response may be approximated by a polynomial as follows:
\begin{equation} \label{eq:5-2.1}
	h(\underline{X}) \, \, \approx \, \, \widehat{h}(\underline{x}) \, \, = \, \, \sum_{j=0}^{P-1} \; a_{j} \; \psi_{j}(\underline{x})
\end{equation}

Let us consider a set of values taken by the input vector (i.e. an experimental design) $\underline{\underline{\mathcal{X}}} = (\underline{x}^{(1)},\dots,\underline{x}^{(N)})^{\textsf{T}}$ as well as the vector $\underline{\mathcal{Y}} = (h(\underline{x}^{(1)}),\dots,h(\underline{x}^{(N)}))^{\textsf{T}} =  (y^{(1)},\dots,y^{(N)})^{\textsf{T}}$ of the corresponding model evaluations. It is assumed that the number of terms $P$ in the polynomial basis is of similar size to $N$, or even possibly significantly larger than $N$. In such a situation it is not possible to compute the polynomial coefficients by ordinary least squares, since the corresponding system is ill-posed. Methods that may be employed as an alternative are outlined in the sequel.  \\

\textbf{LASSO} \vspace{2mm}

The so-called \emph{LASSO} (for \emph{Least absolute shrinkage and selection operator}) method is based on a $\mathcal{L}^{1}$-penalized regression as follows:
\begin{equation} \label{eq:5-2.5}
		\textrm{Minimize} \quad \quad \sum_{i=1}^{N} \; \left( h(\underline{x}^{(i)}) \; - \; \underline{a}^{\textsf{T}} \underline{\psi}(\underline{x}^{(i)})  \right)^{2}
\, + \,  C \; \|\underline{a}\|_{1}^{2}
\end{equation} 
where $\|\underline{a}\|_{1} = \sum_{j=0}^{P-1} |a_{j}|$ and $C$ is a non negative constant. A nice feature of LASSO is that it provides a \emph{sparse} metamodel, i.e. it discards insignificant variables from the set of predictors. The obtained response surface is all the sparser since the value of the tuning parameter $C$ is high. \\

For a given $C\geq 0$, the solving procedure may be implemented via quadratic programming. Obtaining the whole set of coefficient estimates for $C$ varying from 0 to a maximum value may be computationally expensive though since it requires solving the optimization problem for a fine grid of values of $C$.  \\

\textbf{Forward stagewise regression} \vspace{2mm}

Another procedure, known as \emph{forward stagewise regression}, appears to be different from LASSO, but turns out to provide similar results. The procedure first picks the predictor that is most correlated with the vector of observations. However, the value of the corresponding coefficient is only increased by a small amount. Then the predictor with largest correlation with the current residual (possible the same term as in the previous step) is picked, and so on. Let us introduce the vector notation:
\begin{equation} 
	\underline{\psi}_{j} \, \, = \, \, (\psi_{j}(\underline{x}^{(1)}) , \dots, \psi_{j}(\underline{x}^{(N)}) )^{\textsf{T}}
\end{equation} 

The forward stagewise algorithm is outlined below:
\begin{enumerate}
	\item Start with $\underline{R} = \mathcal{Y}$ and $a_{0} = \dots = a_{P-1} = 0$.
	\item Find the predictor $\underline{\psi}_{j}$ that is most correlated with $\underline{R}$.
	\item Update $\hat{a}_{j} = \hat{a}_{j} + \delta_{j}$, where $\delta_{j} = \epsilon \cdot \mbox{sign}(\underline{\psi}_{j}^{\textsf{T}} \underline{R} )$.
	\item Update $\underline{R} =  \underline{R} - \delta_{j} \underline{\psi}_{j}$ and repeats Steps~2 and 3 until no predictor has any correlation with $\underline{R}$.
\end{enumerate}  
Note that parameter $\epsilon$ has to be set equal to a small value in practice, say $\epsilon=0.01$. This procedure is known to be more stable than traditional stepwise regression. \\

\textbf{Least Angle Regression} \vspace{2mm}

\emph{Least Angle Regression} (LAR) may be viewed as a version of forward stagewise that uses mathematical derivations to speed up the computations. Indeed, instead of taking many small steps with the basis term most correlated with current residual $\underline{R}$, the related coefficient is directly increased up to the point where some other basis predictor has as much correlation with $\underline{R}$. Then the new predictor is entered, and the procedure is continued. The LAR algorithm is detailed below:
\begin{enumerate}
	\item Initialize the coefficients to $a_{0},\dots,a_{P-1} = 0$. Set the initial residual equal to the vector of observations $\mathcal{Y}$. 
	\item Find the vector $\underline{\psi}_{j}$ which is most correlated with the current residual.
	\item Move $a_{j}$ from 0 toward the least-square coefficient of the current residual on $\underline{\psi}_{j}$, until some other predictor $\underline{\psi}_{k}$ has as much correlation with the current residual as does $\underline{\psi}_{j}$.
	\item Move jointly $(a_{j} , a_{k})^{\textsf{T}}$ in the direction defined by their joint least-square coefficient of the current residual on $(\underline{\psi}_{j},\underline{\psi}_{k})$, until some other predictor $\underline{\psi}_{l}$ has as much correlation with the current residual. 
	\item Continue this way until $m = \min(P,N-1)$ predictors have been entered. 
\end{enumerate}   
Steps 2 and 3 correspond to a ``move'' of the \emph{active} coefficients toward their least-square value. It corresponds to an updating of the form $\hat{\underline{a}}^{(k+1)} = \hat{\underline{a}}^{(k)} + \gamma^{(k)} \tilde{\underline{w}}^{(k)}$. Vector $\tilde{\underline{w}}^{(k)}$ and coefficient $\gamma^{(k)}$ are referred to as the LAR \emph{descent direction} and \emph{step}, respectively. Both quantities may be derived algebraically. \\

Note that if $N \geq P$, then the last step of LAR provides the ordinary least-square solution. It is shown that LAR is noticeably efficient since it only requires the computational cost of ordinary least-square regression on $P$ predictors for producing a \emph{collection} of $m$ metamodels. \\

\textbf{LASSO and Forward Stagewise as variants of LAR} \vspace{2mm}

It has been shown that with only one modification, the LAR procedure provides in one shot the entire paths of LASSO solution coefficients as the tuning parameter $C$ in Eq.(\ref{eq:5-2.5}) is increased from 0 up to a maximum value. The modified algorithm is as follows:
\begin{itemize}
	\item Run the LAR procedure from Steps~1 to 4.
	\item If a non zero coefficient hits zero, discard it from the current metamodel and recompute the current joint least-square direction.
	\item Continue this way until $m = \min(P,N-1)$ predictors have been entered.
\end{itemize}
Note that the LAR-based LASSO procedure may take more than $m$ steps since the predictors are allowed to be discarded and introduced later again into the metamodel. In a similar fashion, a limiting version of the forward stagewise method when $\epsilon \to 0$ may be obtained by slightly modifying the original LAR algorithm. In the literature, one commonly uses the label LARS when referring to all these LAR-based algorithms (with \emph{S} referring to \emph{Stagewise} and \emph{LASSO}). \\

\textbf{Selection of the optimal LAR solution} \vspace{2mm}

The LAR-based approaches (i.e. original LAR, LASSO and forward stagewise) provide a \emph{collection} of less and less sparse PC approximations. The accuracy of each PC metamodel may be assessed by cross validation \otref{docref_SurfRep_ErrorEstim}{Error estimation based on cross-validation}. Eventually the PC representation associated with the lowest error estimate is retained.
}


\Methodology{
Within the global methodology, the method is used to build up a deterministic polynomial approximation of the model response prior to tackling the step C: ``Uncertainty propagation''. It requires an experimental design together with the corresponding model evaluations. Then the various uncertainty propagation techniques may be applied at a negligible computational cost. 
}
{
Provided that the model under consideration is sufficiently smooth, linear or quadratic polynomials may be accurate approximations. This is especially true for studying the model response not too far from its mean value, i.e. for a central trend analysis. Nonetheless, one should be careful when the estimation of the probability of exceeding a threshold is of interest, since the polynomial approximation in the tails of the response distribution may reveal poor. \\

The principles of the original LAR procedure are detailed in:
\begin{itemize}
  \item B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani, 2004, ``Least angle regression'', Annals of
Statistics 32, 407--499.
\end{itemize}
Furthermore, the modifications of LAR allowing one to solve the LASSO and the forward stagewise problems are presented in:
\begin{itemize}
  \item T. Hastie, J. Taylor, R. Tibshirani, and G. Walther, 2007, ``Forward stagewise regression and
the monotone Lasso'', Electronic J. Stat. 1, 1--29.
\end{itemize}
}


