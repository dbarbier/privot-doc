% Copyright (c)  2005-2010 EDF-EADS-PHIMECA.
% Permission is granted to copy, distribute and/or modify this document
% under the terms of the GNU Free Documentation License, Version 1.2
% or any later version published by the Free Software Foundation;
% with no Invariant Sections, no Front-Cover Texts, and no Back-Cover
% Texts.  A copy of the license is included in the section entitled "GNU
% Free Documentation License".
\renewcommand{\etapemethodo}{B}
\renewcommand{\nomfichier}{docref_B202_Smirnov}
\renewcommand{\titrefiche}{Comparison of two samples using Smirnov test}

\Header

\MathematicalDescription{

  \underline{\textbf{Goal}} \vspace{2mm}

  Let $X$ be a scalar uncertain variable modelled as a random variable. This method is concerned with the construction of a dataset prior to the choice of a probability distribution for $X$. Smirnov's test is a tool that may be used to compare two samples $\left\{x_1,\ldots,x_N \right\}$ and $\left\{x'_1,\ldots,x'_M \right\}$~; the goal is to determine whether these two samples come from the same probability distribution or not. If this is the case, the two samples should be aggregated in order to increase the robustness of further statistical analyses.

  \vspace{2mm}

  \underline{\textbf{Principle of the method}} \vspace{2mm}

  Smirnov's test is a statistical test based on the maximum distance between the cumulative distribution function $\widehat{F}_N$ and $\widehat{F}'_M$ of the samples $\left\{x_1,\ldots,x_N \right\}$ and $\left\{x'_1,\ldots,x'_M \right\}$ (see \otref{docref_B11_EmpiricalCDF}{empirical cumulative distribution function}). This distance is expressed as follows:
  $$
  \widehat{D}_{M,N} = \sup_x \left|\widehat{F}_N\left(x\right) - \widehat{F}'_M\left(x\right)\right|
  $$

  The probability distribution of the distance $\widehat{D}_{M,N}$ is asymptotically known (i.e. as the size of the samples tends to infinity). If $M$ and $N$ are sufficiently large, this means that for a probability $\alpha$, one can calculate the threshold / critical value $d_\alpha$ such that:
  \begin{itemize}
  \item if  $\widehat{D}_{M,N} >d_{\alpha}$, we conclude that the two samples are not identically distributed, with a risk of error $\alpha$,
  \item if  $\widehat{D}_{M,N} \leq d_{\alpha}$, it is reasonable to say that both samples arise frome the same distribution.
  \end{itemize}

  An important notion is the so-called "$p$-value" of the test. This quantity is equal to the limit error probability $\alpha_\textrm{lim}$ under which the "identically-distributed" hypothesis is rejected. Thus, the two samples will be supposed identically distributed if and only if $\alpha_\textrm{lim}$ is greater than the value $\alpha$ desired by the user. Note that the higher $\alpha_\textrm{lim} - \alpha$, the more robust the decision.

  \vspace{2mm}
}
{
  This test is also referred to as the Kolmogorov-Smirnov's test for two samples.
}

\Methodology{
  This method is used in step B "Quantifying Sources of Uncertainty". It is a tool for the construction of a dataset that can be used afterwards to choose  a probability distribution for some uncertain variables defined in step A "Specifying Criteria and the Case Study".
}
{
  The test is concerned with the maximum deviation between the tw empirical distributions; it is by nature highly sensitive to presence of local deviations (two samples may be rejected even if they seem similar for almost the whole domain of variation).\\

  We remind the reader that the underlying theoretical results of the test are asymptotic. There is no rule to determine the minimum number of data values one needs to use this test; but it is often considered a reasonable approximation when $N$ is of an order of a few dozen.

  The following bibliographical references provide main starting points for further study of this method:
  \begin{itemize}
  \item Saporta G. (1990). "Probabilités, Analyse de données et Statistique", Technip
  \item Dixon W.J. \& Massey F.J. (1983) "Introduction to statistical analysis (4th ed.)", McGraw-Hill
  \end{itemize}
}
