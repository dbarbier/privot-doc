%Copyright (c)  2005-2010 EDF-EADS-PHIMECA.
%  Permission is granted to copy, distribute and/or modify this document
%  under the terms of the GNU Free Documentation License, Version 1.2
%  or any later version published by the Free Software Foundation;
%  with no Invariant Sections, no Front-Cover Texts, and no Back-Cover
%  Texts.  A copy of the license is included in the section entitled "GNU
%  Free Documentation License".
\renewcommand{\etapemethodo}{C}
\renewcommand{\nomfichier}{docref_C322_TI}
\renewcommand{\titrefiche}{Importance Simulation}

\Header

\MathematicalDescription{
\underline{\textbf{Goal}}\\
Let us note $\mathcal D_f = \{\ux \in \mathbb R^{n_X} | g(\ux,\underline{d}) \leq 0\}$.
The goal is to estimate the following probability:
\begin{eqnarray*}
        P_f &=& \int_{\mathcal D_f} f_{\uX}(\ux)d\ux\\
            &=& \int_{\mathbb R^{n_X}} \mathbf{1}_{\{g(\ux,\underline{d}) \:\leq 0\: \}}f_{\uX}(\ux)d\ux\\
            &=& \Prob {\{g(\uX,\underline{d}) \leq 0\}}
\end{eqnarray*}

\underline{\textbf{Principles}}\\
        This is a sampling-based method. The main idea of the Importance Sampling method is to replace the initial probability distribution of the input variables by a more "efficient" one. "Efficient" means that more events will be counted in the failure domain $\mathcal D_f$ and thus reduce the variance of the estimator of the probability of exceeding a threshold. Let $\underline{Y}$ be a random vector such that its probability density function $f_{\underline{Y}}(\underline{y}) > 0$ almost everywhere in the domain $\mathcal D_f$,

\begin{eqnarray*}
P_f &=& \int_{\mathbb R^{n_X}} \mathbf{1}_{\{g(\ux,\underline{d}) \leq 0 \}}f_{\uX}(\ux)d\ux\\
    &=& \int_{\mathbb R^{n_X}} \mathbf{1}_{\{g(\ux,\underline{d}) \leq 0 \}} \frac{f_{\uX}(\ux)}{f_{\underline{Y}}(\ux)}f_{\underline{Y}}(\ux)d\ux
\end{eqnarray*}

The estimator built by Importance Sampling method is:
$$
	\hat{P}_{f,IS}^N = \frac{1}{N}\sum_{i=1}^N \mathbf{1}_{\{g(\underline{Y}_{\:i}),\underline{d}) \leq 0 \}}\frac{f_{\uX}(\underline{Y}_{\:i})}{f_{\underline{Y}}(\underline{Y}_{\:i})}
$$
where:
	\begin{itemize}
		\item $N$ is the total number of computations,
		\item	the random vectors $\{\underline{Y}_i, i=1\hdots N\}$ are independent, identically distributed and following the probability density function $f_{\uY}$
	\end{itemize}
	
\underline{\textbf{Confidence Intervals}}\\
With the notations,
\begin{eqnarray*}
	\mu_N &=& \frac{1}{N}\sum_{i=1}^N \mathbf{1}_{\{g(\underline{y}_{\:i}),\underline{d}) \leq 0 \}}\frac{f_{\uX}(\underline{y}_{\:i})}{f_{\underline{Y}}(\underline{y}_{\:i})}\\
	\sigma_N^2 &=& \frac{1}{N}\sum_{i=1}^N (\mathbf{1}_{\{g(\underline{y}_i),\underline{d}) \leq 0 \}}\frac{f_{\uX}(\underline{y}_{\:i})}{f_{\underline{Y}}(\underline{y}_{\:i})} - \mu_N)^2
\end{eqnarray*}

The asymptotic confidence interval of order $1-\alpha$ associated to the estimator $P_{f,IS}^N$ is 
$$
[ \mu_N - \frac{q_{1-\alpha / 2} . \sigma_N}{\sqrt{N}} \: ; \: \mu_N + \frac{q_{1-\alpha / 2} . \sigma_N}{\sqrt{N}} ]
$$

where $q_{1-\alpha /2}$ is the $1-\alpha / 2$ quantile from the standard distribution $\mathcal N(0,1)$.

\index{tirage d'importance}
}
{
% Autres notations et appellations
This method could also be found under the name "Strategic Sampling", "Ponderated Sampling" or "Biased Sampling" (even if this estimator is not biased as it gives exactly the same result).
}

\Methodology{
This method is part of the step C of the global methodology. It requires the specification the joined probability density function of the input variables and the value of the threshold and the comparison operator.\\
}
{
There is no general result concerning the reduction of variance of $\hat{P}_{f,IS}^N$ in comparison with the variance of the initial Monte Carlo estimator $\hat{P}_{f,MC}^N$. Nevertheless, if one knows well the model (regularity, monotoneous,...), it is possible to define a more efficient joined probability density function.
Nevertheless, there is a reduction of variance if one chooses a density $f_{\underline{Y}}(\underline{y})$ such that $f_{\underline{Y}}(\underline{y})> f_{\underline{X}}(\underline{y})$ almost everywhere in the failure space. Indeed, in this case $\frac{f_{\uX}(\underline{y})}{f_{\underline{Y}}(\underline{y})} < 1$ on all the domain, the variance being equal to:
$$
\Var {\hat{P}_{f,IS}} = \int_{\mathcal D_f} \left( \frac{f_{\uX}(\underline{y})}{f_{\underline{Y}}(\underline{y})} \right)^2 d\underline{y} - P_f^2 \ \ < \ \ \Var {\hat{P}_{f,MC}} = P_f-P_f^2
$$

Moreover, one has to pay attention to define the same support for the joined pdf of the input variables to ensure the convergence.


The following references are a first introduction to the Monte Carlo methods:

W.G. Cochran. \textit{Sampling Techniques}. John Wiley and Sons, 1977.

M.H. Kalos et P.A. \textit{Monte Carlo Methods, volume I: Basics}. John Wiley and Sons, 1986.

R.Y. Rubinstein. \textit{Simulation and the Monte Carlo Method}. John Wiley and Sons, 1981.

Autres r�f�rences � int�grer
 
}

%\Example{
%
%	Knowing that $X \hookrightarrow \mathcal N(0,1)$ and $Y \hookrightarrow \mathcal N(0,1)$, we want to compute:
%	$$
%		\mathbb P_f = \mathbb E(\mathbf 1_{X+Y > s}) = \int_{\mathbb R^2}\mathbf 1_{x+y \leq s}\:\frac{1}{2\pi}e^{-{\frac{x^2+y^2}{2}}}\:dxdy
%	$$
%	
%%	\begin{figure}
%%		\centering
%%			\includegraphics[width=0.85\textwidth]{E_comp_MC_TI.eps}
%%		\label{fig:Ex_comp_MC_TI}
%%	\end{figure}
%
%In this case, the failure domain is the domain $D_2$ from the previous figure. Let $\mathcal E$ be the function defined by:
%$$
%	\mathcal E(s) = \frac{Var(P_{f,IS}^N)}{Var(P_{f,MC}^N)}
%$$
%
%Let$\mathcal E$ is a measure of the efficiency of the Importance Sampling method vs the crude Monte Carlo method.
%
%\begin{itemize}
%	\item
%		When $\mathcal E \leq 1$, it means that the variance of the estimator built by the Importance Sampling method is smaller than the variance of the estimator obtained by a crude Monte Carlo method. In this case, the Importance Sampling method is better than the crude Monte Carlo method (which means that the asymptotic Confidence Interval is smaller).
%	\item
%			When $\mathcal E \geq 1$, it means that the variance of the estimator built by the Importance Sampling method is greater than the variance of the estimator obtained by a crude Monte Carlo method. In this case, the Importance Sampling method is worse than the crude Monte Carlo method (which means that the asymptotic Confidence Interval is bigger).
%\end{itemize}
%
%ONe can show that:
%
%$$
%	\mathcal E(s) = \frac{e^{\frac{s^2}{2}}+\frac{1}{2}.erf(s)-\frac{1}{2}}{\frac{1}{2}+\frac{1}{2}.erf(s)}
%$$
%where $erf(s) = \frac{2}{\sqrt{\pi}}\int_0^s\:e^{-t^2}\:dt $ 
%
%
%%\begin{figure}
%%	\centering
%%		\includegraphics[width=0.80\textwidth]{Pf_vs_s.eps}
%%	\label{fig:Pf}
%%\end{figure}
%
%The more s increases, the more the value of $P_f$ decreases (the domain $D_2$ is smaller and smaller), which is shown in the previous figure.
%
%
%%\begin{figure}
%%	\centering
%%		\includegraphics[width=0.80\textwidth]{Eff_IS_MC_P2_vs_S.eps}
%%	\label{fig:Var_s_P1}
%%\end{figure}
%
%This curve shows the efficiency $\mathcal E$ vs $s$.
%
%%\begin{figure}
%%	\centering
%%		\includegraphics[width=0.80\textwidth]{Eff_IS_MC_P2_vs_Pf.eps}
%%	\label{fig:Var_s_P1}
%%\end{figure}
%This curve shows the efficiency $\mathcal E$ vs $P_f$.
%}


