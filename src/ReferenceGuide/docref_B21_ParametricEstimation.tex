% Copyright (c)  2005-2010 EDF-EADS-PHIMECA.
% Permission is granted to copy, distribute and/or modify this document
% under the terms of the GNU Free Documentation License, Version 1.2
% or any later version published by the Free Software Foundation;
% with no Invariant Sections, no Front-Cover Texts, and no Back-Cover
% Texts.  A copy of the license is included in the section entitled "GNU
% Free Documentation License".
\renewcommand{\etapemethodo}{B}
\renewcommand{\nomfichier}{docref_B21_ParametricEstimation}
\renewcommand{\titrefiche}{Parametric Estimation}

\Header

\MathematicalDescription{

  \underline{\textbf{Goal}} \vspace{2mm}

  The objective is to estimate the value of the parameters based on a sample of an unknown distribution, supposed to be a member of a parametric family of distributions. We describes here the estimators implemented in OpenTURNS for the estimation of the several parametric models. They are all derived from either the Maximum Likelihood method or from the method of moments, excepted for the bound parameters that are systematically modified to strictly include the extrem realizations of the underlying sample $(x_1,\dots,x_n)$.

  We suppose that we have a realization $(\vect{x}_1,\dots,\vect{x}_n)$ of a sample $(\vect{X}_1,\dots,\vect{X}_n)$ of size $n$, with the $X_i$ being iid, with common distribution $\mathcal{D}(\vect{\theta})$. The objective is to build an estimator $\Hat{\theta}_n$ of $\vect{\theta}$, based on the realization $(\vect{x}_1,\dots,\vect{x}_n)$. We adopt the following notations:
  \begin{itemize}
  \item $\bar{\vect{x}}_n=\frac{1}{n}\sum_{i=1}^n\vect{x}_i$ the sample mean ($\bar{x}_n$ in the 1D case);
  \item $\sigma_n^X=\sqrt{\frac{1}{n-1}\sum_{i=1}^n(x_i-\bar{x})^2}$ the sample standard deviation in the 1D case;
  \item $x_{(1,n)}=\min_{i=1,\dots,n}x_i$ the minimum of the realization in the 1D case;
  \item $x_{(n,n)}=\max_{i=1,\dots,n}x_i$ the maximum of the realization in the 1D case;
  \item $x_{1/2}$ the median of the sample in the 1D case;
  \end{itemize}

  \underline{\textbf{Continuous univariate distributions:}}\\

  \begin{tabular}{|l|p{12cm}|}
    \hline
    Arcsine & $\begin{array}{l}
      \displaystyle\Hat{\mu} = \Hat{\mu}_x\\
      \displaystyle\Hat{\sigma} = \Hat{\sigma}_x\\
    \end{array}$\\
    \hline
    Beta & $\begin{array}{l}
      \displaystyle\Hat{a}_n=(1-\mathrm{sign}(x_{(1,n)})/(2+n))x_{(1,n)}\\
      \displaystyle\Hat{b}_n=(1+\mathrm{sign}(x_{(n,n)})/(2+n))x_{(n,n)}\\
      \displaystyle\Hat{t}_n=\frac{(\Hat{b}_n-\bar{x}_n)(\bar{x}_n-\Hat{a}_n)}{(\sigma_n^X)^2-1}\\
      \displaystyle\Hat{r}_n=\frac{t(\bar{x}_n-\Hat{a}_n)}{\Hat{b}_n-\Hat{a}_n}
    \end{array}$\\
    \hline
    Burr & $\Hat{c}_n$ is the solution of the non linear equation : 
$$
\displaystyle 1+\frac{c}{n}\left[ SR - \frac{n}{\sum_{i=1}^n \log(1+x_i^c)}SSR\right] = 0
$$
where $ \displaystyle SR = \displaystyle \sum_{i=1}^n \frac{ \log(x_i)}{1+x_i^c}$ and $ \displaystyle SSR = \displaystyle \sum_{i=1}^n \frac{ x_i^c\log(x_i)}{1+x_i^c}$.
Then 
$$\Hat{k}_n =  \frac{n}{\sum_{i=1}^n \log(1+x_i^c)}.
$$


\\
    \hline
  \end{tabular}

  \begin{tabular}{|l|p{12cm}|}
    \hline
    Chi & $\displaystyle\Hat{\nu}_n=\bar{x^2}_n$\\
    \hline
    ChiSquare & $\displaystyle\Hat{\nu}_n=\bar{x}_n$\\
    \hline
    Dirichlet &  Maximum likelihood estimators, according to the reference J. Huang\\
    \hline
    Epanechnikov  & see the Beta distribution\\
    \hline
    Exponential & $\begin{array}{l}
      \displaystyle\Hat{\gamma}_n = (1-\mathrm{sign}(x_{(1,n)})/(2+n))x_{(1,n)}\\
      \displaystyle\Hat{\lambda}_n= 1/\bar{x}_n-\Hat{\gamma}_n
    \end{array}$\\
    \hline
    Fisher-Snedecor   & No factory method implemented so far\\
    \hline
    Gamma & $\begin{array}{l}
      \displaystyle\Hat{\gamma}_n = (1-\mathrm{sign}(x_{(1,n)})/(2+n))x_{(1,n)}\\
      \displaystyle\Hat{\lambda}_n= \frac{\bar{x}_n-\Hat{\gamma}_n}{(\sigma_n^X)^2}\\
      \displaystyle\Hat{k}_n= \frac{(\bar{x}_n-\Hat{\gamma}_n)^2}{(\sigma_n^X)^2}
    \end{array}$\\
    \hline
    Gumbel &  $\begin{array}{l}
      \displaystyle\Hat{\alpha}_n=\frac{\pi}{\sigma_n^X\sqrt{6}}\\
      \displaystyle\Hat{\beta}_n=\bar{x}_n-\frac{\gamma\sqrt{6}}{\pi}\sigma_n^X\\
      \gamma\simeq 0.57721\mbox{ is Euler's constant.}
    \end{array}$\\
    \hline
    Histogram & The bandwidth is the AMISE-optimal one : $h = \displaystyle \frac{(24\sqrt{\pi})^{1/3} \sigma_n}{n^{1/3}}$ where $\sigma_n^2$ is the non biaised variance of the data. The range is $[min(data), max(data)]$.\\
    \hline
     InverseNormal   & $\begin{array}{l}
      \displaystyle\Hat{\mu}_n =  \bar{x}_n\\
      \displaystyle\Hat{\lambda}_n = \left(  \frac{1}{n} \sum_{i=1}^n \frac{1}{x_i} - \frac{1}{\bar{x}_n} \right)^{-1}
    \end{array}$\\
    \hline
    Laplace & $\begin{array}{l}
      \displaystyle\Hat{\mu}_n = x_{1/2}\\
      \displaystyle\Hat{\lambda}_n = \frac{1}{n}\sum_{i=1}^n|x_i-\Hat{\mu}_n|
    \end{array}$\\
    \hline
    Logistic & $\begin{array}{l}
      \displaystyle\Hat{\alpha} = \bar{x}_n\\
      \displaystyle\Hat{\beta}_n = \sigma_n^X
    \end{array}$\\
    \hline
    LogNormal & see text below\\
    \hline
    LogUniform & $\begin{array}{l}
      \displaystyle\Hat{a}_n=(1-1/(2+n))x_{(1,n)}\\
      \displaystyle\Hat{b}_n=(1+1/(2+n))x_{(n,n)}
    \end{array}$\\
    \hline
    Non Central Chi Square   & No factory method implemented so far\\
    \hline
    Non Central Student   & No factory method implemented so far\\
    \hline
    Rayleigh & $\begin{array}{l}
      \displaystyle\Hat{\gamma}_n = (1-\mathrm{sign}(x_{(1,n)})/(2+n))x_{(1,n)}\\
      \displaystyle\Hat{\sigma}_n=\sqrt{\frac{2}{n}\sum_{i=1}^n(x_i-\Hat{\gamma}_n)^2}
    \end{array}$\\
    \hline
    Rice   & No factory method implemented so far\\
    \hline
    Trapezoidal   & Numerical resolution of maximum likelihood estimators\\
    \hline
    Triangular & $\begin{array}{l}
      \displaystyle\Hat{a}_n=(1-\mathrm{sign}(x_{(1,n)})/(2+n))x_{(1,n)}\\
      \displaystyle\Hat{b}_n=(1+\mathrm{sign}(x_{(n,n)})/(2+n))x_{(n,n)}\\
      \displaystyle\Hat{m}_n=3\bar{x}_n-\Hat{a}_n-\Hat{b}_n
    \end{array}$\\
    \hline
  \end{tabular}\rule{0pt}{1em}\\




  \begin{tabular}{|l|p{12cm}|}
    \hline
    TruncatedNormal & Numerical maximum likelihood estimation. \\
    \hline
    Uniform & $\begin{array}{l}
      \displaystyle\Hat{a}_n=(1-\mathrm{sign}(x_{(1,n)})/(2+n))x_{(1,n)}\\
      \displaystyle\Hat{b}_n=(1+\mathrm{sign}(x_{(n,n)})/(2+n))x_{(n,n)}
    \end{array}$\\
    \hline
    Weibull & $\begin{array}{l}
      \displaystyle\Hat{\gamma}_n = (1-\mathrm{sign}(x_{(1,n)})/(2+n))x_{(1,n)}\\
      (\Hat{\alpha}_n,\Hat{\beta}_n)\mbox{ solution of }\left\{\begin{array}{l}
          \bar{x}_n=\Hat{\gamma}_n+\Hat{\alpha}_n+\Gamma(1+1/\Hat{\beta}_n)\\
          (\sigma_n^X)^2=\Hat{\alpha}_n\left(\Gamma(1+2/\Hat{\beta}_n)-\Gamma(1+1/\Hat{\beta}_n)^2\right)
        \end{array}\right.
    \end{array}$\\
    \hline
  \end{tabular}\rule{0pt}{1em}\\


  Details for the logNormal distribution : \\

  We define the sums :
  \begin{eqnarray}
    \displaystyle S_0 & = & \sum_{i=1}^n \frac{1}{x_i - \gamma} \nonumber \\
    \displaystyle S_1 & = & \sum_{i=1}^n log(x_i - \gamma)  \nonumber \\
    \displaystyle S_2 &  = & \sum_{i=1}^n log^2(x_i - \gamma)  \nonumber \\
    \displaystyle S_3 & = & \sum_{i=1}^n \frac{log(x_i - \gamma)}{x_i - \gamma} \nonumber
  \end{eqnarray}

  The Maximum Likelihood estimators of $(\mu^{log}, \sigma^{log}, \gamma)$ are :
  \begin{eqnarray}
    \displaystyle\Hat{\mu}^{log}_n & = & \frac{S_1}{n}  \label{MLEMu3} \\
    \displaystyle\Hat{\sigma}^{log}_n & = & \sqrt{\frac{S_2}{S_4} - \left(\Hat{\mu}^{log}_n\right)^2}  \label{MLESigma3}\\
    \displaystyle \Hat{\gamma}_n &  = & S_0(\left(\Hat{\sigma}^{log}_n\right)^2 - \Hat{\mu}^{log}_n) + S_3
  \end{eqnarray}

  Thus, $\Hat{\gamma}_n$ verifies the relation :
  \begin{equation} \label{relaGamma}
    S_0(S_2-S_1(1+\frac{S_1}{S_4}))+S_4S_3 = 0
  \end{equation}
  Open TURNS tries to solve (\ref{relaGamma})  by the step doubling bracheting method followed by the bisection method. Once $\Hat{\gamma}_n$ is evaluated, $(\Hat{\mu}^{log}_n, \Hat{\sigma}^{log}_n)$ are evaluated as defined in (\ref{MLEMu3}) and (\ref{MLESigma3}).\\
  If not possible, Open TURNS evaluates $\Hat{\gamma}_n$ as :
  $$
  \displaystyle \Hat{\gamma}_n = min(x_i)_{1 \leq i \leq n} - \epsilon \sqrt{\sigma_n^{*}}
  $$
  where $ \displaystyle M_n = \frac{1}{n}\sum_{i=1}^n x_i$ and $ \displaystyle \sigma_n^{*} = \sqrt{\frac{1}{n-1}\sum_{i=1}^n (x_i - M_n)^2}$ and $\epsilon = 10^{-12}$. Then, it evaluates $(\Hat{\mu}^{log}_n, \Hat{\sigma}^{log}_n)$ with the Maximum Likelihood estimators when $\gamma$ is known :
  \begin{eqnarray}
    \displaystyle\Hat{\mu}^{log}_n = \frac{1}{n}\sum_{i=1}^n\log(x_i-\Hat{\gamma}_n)\\
    \displaystyle\Hat{\sigma}^{log}_n=\sqrt{\frac{1}{n-1}\sum_{i=1}^n\left(\log(x_i-\Hat{\gamma}_n) - \Hat{\mu}^{log}_n\right)^2}
  \end{eqnarray}




  \underline{\textbf{Discrete univariate distributions :}}\\

  \begin{tabular}{|l|p{12cm}|}
    \hline
    Bernoulli & $\Hat{p}_n = \bar{x}_n$\\
    \hline
    Binomial & See details below.\\
    \hline
    Geometric & $\Hat{p}_n = \bar{x}_n$\\
    \hline
    Multinomial &
    $      \begin{array}{l}
      data : (\vect{x}^1, \hdots,\vect{x}^n)\\
      \displaystyle  N = max_{i,k} \, x_i^k \\
      \displaystyle  p_i = \frac{1}{nN} \sum_{k=1}^{n} x_i^k
    \end{array}$\\
    \hline
    Negative Binomial & No factory method implemented so far\\
    \hline
  \end{tabular}\rule{0pt}{1em}\\

  \begin{tabular}{|l|p{12cm}|}
    \hline
    Poisson & $\displaystyle\Hat{\lambda}_n = \bar{x}_n$\\
    \hline
    Rice   & No factory method implemented so far\\
    \hline
    UserDefined & Uniform distribution over the sample.\\
    \hline
  \end{tabular}\rule{0pt}{1em}\\


  Details for the Binomial distribution : \\
  We initialize the value of $(n,p_n)$ to $(\displaystyle \frac{M_N^2}{M_N-V_N}, \displaystyle \frac{1}{n}M_N)$ where $M_n$ is the empirical mean of the sample $(x_1, \hdots, x_N)$, and $V_N$ its unbiaised empirical variance, obtained from the moments estimation method.\\
  Then, we evaluate the likelihood of the sample with respect to the Binomial distribution parameterized with $(\displaystyle \frac{M_N^2}{M_N-V_N}, \displaystyle \frac{1}{n}M_N)$. By testing successively $n+1$ and $n-1$ instead of $n$, we determine the variation of the likelihood of  the sample  with respect to the Binomial distribution parameterized with  $(n+1,p_{n+1})$ and $(n-1,p_{n-1})$. We then interate in the sense which makes the likelihood decreasing, until the likelihood stops decreasing. The last couple is the one selected.\rule{0pt}{1em}\\


  \newpage

  \underline{\textbf{Copula distributions :}}\\

  \begin{tabular}{|l|p{12cm}|}
    \hline
    ClaytonCopula & $\displaystyle\Hat{\theta}_n=\frac{2\Hat{\tau}_n^{\strut}}{1_{\strut} - \Hat{\tau}_n}$ where $\Hat{\tau}_n$ is the Kendall-$\tau$ of the sample.\\
    \hline
    FrankCopula & $\Hat{\theta}_n$ solution of $\displaystyle \Hat{\tau}_n = 1-4\left( \frac{1-D(\Hat{\theta}_n, 1)^{\strut}}{\theta} \right)$ where $\Hat{\tau}_n$ is the Kendall-$\tau$ of the sample and $D$ is the Debye function defined as $\displaystyle D(x, n)=\frac{n}{x^n}\int_0^x \frac{t^n}{e^t-1_{\strut}} dt$.\\
    \hline
    GumbelCopula & $\displaystyle \Hat{\theta}_n=\frac{1^{\strut}}{1 - \Hat{\tau}_{n_{\strut}}}$ where $\Hat{\tau}_n$ is the Kendall-$\tau$ of the sample.\\
    \hline
    NormalCopula &  The Kendall-$\tau$ $\Hat{\tau}_n^{\strut}$ of the sample is evaluated. Then the NormalCopula is parameterized by the $R$ correlation matrix defined as : $R_{ij} = sin(\frac{\pi}{2}\tau_{ij})_{\strut}$.\\
    \hline
    Normal & $\begin{array}{l}
      \displaystyle\Hat{\vect{\mu}}_n^{\strut} = \bar{\vect{x}}_n\\
      \displaystyle\Hat{\mathrm{Cov}}_n = \frac{1}{n-1}\sum_{i=1}^n\left(\vect{X}_i-\Hat{\vect{\mu}}_n\right)\left(\vect{X}_i-\Hat{\vect{\mu}}_n\right)^t
    \end{array}$\\
    \hline
    UserDefined & Uniform distribution over the sample.\\
    \hline
  \end{tabular}


}
{
  --
}


\Methodology{
  When the amount of data is sufficient, parametric estimation may be used within Step B ; Quantification of Uncertainties to model the uncertainty of some input random vectors or the output random vector.
}
{
  The following bibliographical references provide main starting points for further study of  this method:
  \begin{itemize}
  \item Huang J., "Maximum Likelihood estimation of Dirichlet Distribution Parameters".
  \item Saporta G. (1990). "Probabilités, Analyse de données et Statistique", Technip
  \item Dixon W.J. \& Massey F.J. (1983) "Introduction to statistical analysis (4th ed.)", McGraw-Hill
  \end{itemize}
}

